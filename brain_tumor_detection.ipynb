{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "os.environ[\"KERAS__BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Flatten, Dense, MaxPooling2D, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score ,accuracy_score\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir=\"./archive/brain_tumor_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(file_dir, n_generated_samples, save_to_dir, augment_params=None, verbose=False):\n",
    "    augment_params = augment_params or {\n",
    "        'rotation_range': 10,\n",
    "        'width_shift_range': 0.1,\n",
    "        'height_shift_range': 0.1,\n",
    "        'shear_range': 0.1,\n",
    "        'brightness_range': (0.3, 1.0),\n",
    "        'horizontal_flip': True,\n",
    "        'vertical_flip': True,\n",
    "        'fill_mode': 'nearest'\n",
    "    }\n",
    "    \n",
    "    data_gen = ImageDataGenerator(**augment_params)\n",
    "\n",
    "    os.makedirs(save_to_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(file_dir):\n",
    "        file_path = os.path.join(file_dir, filename)\n",
    "        \n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            if verbose:\n",
    "                print(f\"Skipping non-image file: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is None:\n",
    "                if verbose:\n",
    "                    print(f\"Failed to load image: {filename}\")\n",
    "                continue\n",
    "\n",
    "            image = image.reshape((1,) + image.shape)\n",
    "            save_prefix = 'aug_' + os.path.splitext(filename)[0]\n",
    "\n",
    "            for i, batch in enumerate(data_gen.flow(x=image, batch_size=1, \n",
    "                                                     save_to_dir=save_to_dir, \n",
    "                                                     save_prefix=save_prefix, \n",
    "                                                     save_format='jpg')):\n",
    "                if i >= n_generated_samples:\n",
    "                    break\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Augmented {filename} -> {n_generated_samples} samples\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('augmented-images')\n",
    "os.makedirs('augmented-images/yes')\n",
    "os.makedirs('augmented-images/no')\n",
    "augmented_data_path ='./augmented-images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Y105.jpg -> 6 samples\n",
      "Augmented no 90.jpg -> 9 samples\n",
      "Augmented Y154.jpg -> 6 samples\n",
      "Augmented Y75.JPG -> 6 samples\n",
      "Augmented Y61.jpg -> 6 samples\n",
      "Augmented no 94.jpg -> 9 samples\n",
      "Augmented 36 no.jpg -> 9 samples\n",
      "Augmented No18.jpg -> 9 samples\n",
      "Augmented 11 no.jpg -> 9 samples\n",
      "Augmented Y257.jpg -> 6 samples\n",
      "Augmented Y180.jpg -> 6 samples\n",
      "Augmented Y8.jpg -> 6 samples\n",
      "Augmented no 96.jpg -> 9 samples\n",
      "Augmented Y38.jpg -> 6 samples\n",
      "Augmented 37 no.jpg -> 9 samples\n",
      "Augmented Y194.jpg -> 6 samples\n",
      "Augmented 20 no.jpg -> 9 samples\n",
      "Augmented Y147.JPG -> 6 samples\n",
      "Augmented Y92.jpg -> 6 samples\n",
      "Augmented Y255.JPG -> 6 samples\n",
      "Augmented Y254.jpg -> 6 samples\n",
      "Augmented Y9.jpg -> 6 samples\n",
      "Augmented no 98.jpg -> 9 samples\n",
      "Augmented N15.jpg -> 9 samples\n",
      "Augmented No22.jpg -> 9 samples\n",
      "Augmented 41 no.jpg -> 9 samples\n",
      "Augmented 19 no.jpg -> 9 samples\n",
      "Augmented 8 no.jpg -> 9 samples\n",
      "Augmented No16.jpg -> 9 samples\n",
      "Augmented 35 no.jpg -> 9 samples\n",
      "Augmented 27 no.jpg -> 9 samples\n",
      "Augmented Y103.jpg -> 6 samples\n",
      "Augmented 33 no.jpg -> 9 samples\n",
      "Augmented N26.JPG -> 9 samples\n",
      "Augmented 48 no.jpeg -> 9 samples\n",
      "Augmented 14 no.jpg -> 9 samples\n",
      "Augmented Y96.jpg -> 6 samples\n",
      "Augmented Y114.JPG -> 6 samples\n",
      "Augmented Y249.JPG -> 6 samples\n",
      "Augmented Y81.jpg -> 6 samples\n",
      "Augmented No12.jpg -> 9 samples\n",
      "Augmented 15 no.jpg -> 9 samples\n",
      "Augmented Y107.jpg -> 6 samples\n",
      "Augmented Y82.jpg -> 6 samples\n",
      "Augmented Y244.JPG -> 6 samples\n",
      "Augmented no.jpg -> 9 samples\n",
      "Augmented 3 no.jpg -> 9 samples\n",
      "Augmented Y155.JPG -> 6 samples\n",
      "Augmented 26 no.jpg -> 9 samples\n",
      "Augmented Y164.JPG -> 6 samples\n",
      "Augmented no 100.jpg -> 9 samples\n",
      "Augmented Y37.jpg -> 6 samples\n",
      "Augmented Y15.jpg -> 6 samples\n",
      "Augmented 7 no.jpg -> 9 samples\n",
      "Augmented Y156.JPG -> 6 samples\n",
      "Augmented Y247.JPG -> 6 samples\n",
      "Augmented Y47.JPG -> 6 samples\n",
      "Augmented no 8.jpg -> 9 samples\n",
      "Augmented no 95.jpg -> 9 samples\n",
      "Augmented 40 no.jpg -> 9 samples\n",
      "Augmented 12 no.jpg -> 9 samples\n",
      "Augmented Y102.jpg -> 6 samples\n",
      "Augmented 46 no.jpg -> 9 samples\n",
      "Augmented Y69.jpg -> 6 samples\n",
      "Augmented Y58.JPG -> 6 samples\n",
      "Augmented Y92.png -> 6 samples\n",
      "Augmented No20.jpg -> 9 samples\n",
      "Augmented Y183.jpg -> 6 samples\n",
      "Augmented Y169.jpg -> 6 samples\n",
      "Augmented N22.JPG -> 9 samples\n",
      "Augmented Y246.JPG -> 6 samples\n",
      "Augmented no 5.jpeg -> 9 samples\n",
      "Augmented 5 no.jpg -> 9 samples\n",
      "Augmented 50 no.jpg -> 9 samples\n",
      "Augmented N3.jpg -> 9 samples\n",
      "Augmented 25 no.jpg -> 9 samples\n",
      "Augmented 21 no.jpg -> 9 samples\n",
      "Augmented Y46.jpg -> 6 samples\n",
      "Augmented 43 no.jpg -> 9 samples\n",
      "Augmented Y258.JPG -> 6 samples\n",
      "Augmented Y186.jpg -> 6 samples\n",
      "Augmented Y23.JPG -> 6 samples\n",
      "Augmented Y95.jpg -> 6 samples\n",
      "Augmented Y163.JPG -> 6 samples\n",
      "Augmented Y78.jpg -> 6 samples\n",
      "Augmented no 7.jpeg -> 9 samples\n",
      "Augmented Y253.JPG -> 6 samples\n",
      "Augmented no 4.jpg -> 9 samples\n",
      "Augmented Y45.JPG -> 6 samples\n",
      "Augmented Y11.jpg -> 6 samples\n",
      "Augmented Y56.jpg -> 6 samples\n",
      "Augmented Y30.jpg -> 6 samples\n",
      "Augmented Y7.jpg -> 6 samples\n",
      "Augmented No11.jpg -> 9 samples\n",
      "Augmented Y60.jpg -> 6 samples\n",
      "Augmented Y256.JPG -> 6 samples\n",
      "Augmented Y67.JPG -> 6 samples\n",
      "Augmented Y13.jpg -> 6 samples\n",
      "Augmented Y4.jpg -> 6 samples\n",
      "Augmented N11.jpg -> 9 samples\n",
      "Augmented Y24.jpg -> 6 samples\n",
      "Augmented Y115.JPG -> 6 samples\n",
      "Augmented Y148.JPG -> 6 samples\n",
      "Augmented Y245.jpg -> 6 samples\n",
      "Augmented N21.jpg -> 9 samples\n",
      "Augmented Y55.jpg -> 6 samples\n",
      "Augmented N17.jpg -> 9 samples\n",
      "Augmented Y182.JPG -> 6 samples\n",
      "Augmented 23 no.jpg -> 9 samples\n",
      "Augmented Y14.jpg -> 6 samples\n",
      "Augmented 30 no.jpg -> 9 samples\n",
      "Augmented 49 no.jpg -> 9 samples\n",
      "Augmented Y18.JPG -> 6 samples\n",
      "Augmented N20.JPG -> 9 samples\n",
      "Augmented 31 no.jpg -> 9 samples\n",
      "Augmented Y12.jpg -> 6 samples\n",
      "Augmented no 1.jpg -> 9 samples\n",
      "Augmented Y111.JPG -> 6 samples\n",
      "Augmented Y49.JPG -> 6 samples\n",
      "Augmented Y250.jpg -> 6 samples\n",
      "Augmented Y248.JPG -> 6 samples\n",
      "Augmented Y59.JPG -> 6 samples\n",
      "Augmented Y65.JPG -> 6 samples\n",
      "Augmented Y159.JPG -> 6 samples\n",
      "Augmented 1 no.jpeg -> 9 samples\n",
      "Augmented Y195.JPG -> 6 samples\n",
      "Augmented no 2.jpg -> 9 samples\n",
      "Augmented Y62.jpg -> 6 samples\n",
      "Augmented 4 no.jpg -> 9 samples\n",
      "Augmented Y89.JPG -> 6 samples\n",
      "Augmented 24 no.jpg -> 9 samples\n",
      "Augmented Y86.JPG -> 6 samples\n",
      "Augmented Y106.jpg -> 6 samples\n",
      "Augmented Y2.jpg -> 6 samples\n",
      "Augmented no 99.jpg -> 9 samples\n",
      "Augmented Y153.jpg -> 6 samples\n",
      "Augmented 29 no.jpg -> 9 samples\n",
      "Augmented Y21.jpg -> 6 samples\n",
      "Augmented Y100.JPG -> 6 samples\n",
      "Augmented Y158.JPG -> 6 samples\n",
      "Augmented no 97.jpg -> 9 samples\n",
      "Augmented no 6.jpg -> 9 samples\n",
      "Augmented Y76.jpg -> 6 samples\n",
      "Augmented 17 no.jpg -> 9 samples\n",
      "Augmented Y34.jpg -> 6 samples\n",
      "Augmented Y184.JPG -> 6 samples\n",
      "Augmented Y116.JPG -> 6 samples\n",
      "Augmented No15.jpg -> 9 samples\n",
      "Augmented Y109.JPG -> 6 samples\n",
      "Augmented Y22.jpg -> 6 samples\n",
      "Augmented no 91.jpeg -> 9 samples\n",
      "Augmented N16.jpg -> 9 samples\n",
      "Augmented 2 no.jpeg -> 9 samples\n",
      "Augmented Y91.jpg -> 6 samples\n",
      "Augmented Y32.jpg -> 6 samples\n",
      "Augmented Y71.JPG -> 6 samples\n",
      "Augmented Y146.JPG -> 6 samples\n",
      "Augmented Y259.JPG -> 6 samples\n",
      "Augmented no 9.png -> 9 samples\n",
      "Augmented Y41.jpg -> 6 samples\n",
      "Augmented N19.JPG -> 9 samples\n",
      "Augmented Y10.jpg -> 6 samples\n",
      "Augmented Y77.jpg -> 6 samples\n",
      "Augmented Y157.JPG -> 6 samples\n",
      "Augmented no 89.jpg -> 9 samples\n",
      "Augmented no 10.jpg -> 9 samples\n",
      "Augmented No13.jpg -> 9 samples\n",
      "Augmented Y108.jpg -> 6 samples\n",
      "Augmented No21.jpg -> 9 samples\n",
      "Augmented Y192.JPG -> 6 samples\n",
      "Augmented 38 no.jpg -> 9 samples\n",
      "Augmented Y160.JPG -> 6 samples\n",
      "Augmented 42 no.jpg -> 9 samples\n",
      "Augmented 28 no.jpg -> 9 samples\n",
      "Augmented 34 no.jpg -> 9 samples\n",
      "Augmented Y35.jpg -> 6 samples\n",
      "Augmented Y188.jpg -> 6 samples\n",
      "Augmented No19.jpg -> 9 samples\n",
      "Augmented 45 no.jpg -> 9 samples\n",
      "Augmented no 92.jpg -> 9 samples\n",
      "Augmented N1.JPG -> 9 samples\n",
      "Augmented no 923.jpg -> 9 samples\n",
      "Augmented 22 no.jpg -> 9 samples\n",
      "Augmented 32 no.jpg -> 9 samples\n",
      "Augmented Y52.jpg -> 6 samples\n",
      "Augmented 39 no.jpg -> 9 samples\n",
      "Augmented 9 no.jpg -> 9 samples\n",
      "Augmented Y50.JPG -> 6 samples\n",
      "Augmented Y20.jpg -> 6 samples\n",
      "Augmented Y1.jpg -> 6 samples\n",
      "Augmented No14.jpg -> 9 samples\n",
      "Augmented Y33.jpg -> 6 samples\n",
      "Augmented No17.jpg -> 9 samples\n",
      "Augmented N6.jpg -> 9 samples\n",
      "Augmented 18 no.jpg -> 9 samples\n",
      "Augmented no 3.jpg -> 9 samples\n",
      "Augmented N5.jpg -> 9 samples\n",
      "Augmented 47 no.jpg -> 9 samples\n",
      "Augmented 6 no.jpg -> 9 samples\n",
      "Augmented 13 no.jpg -> 9 samples\n",
      "Augmented 10 no.jpg -> 9 samples\n",
      "Augmented 44no.jpg -> 9 samples\n",
      "Augmented N2.JPG -> 9 samples\n",
      "Augmented Y162.jpg -> 6 samples\n",
      "Augmented Y40.JPG -> 6 samples\n",
      "Augmented Y168.jpg -> 6 samples\n",
      "Augmented Y170.JPG -> 6 samples\n",
      "Augmented Y53.jpg -> 6 samples\n",
      "Augmented Y25.jpg -> 6 samples\n",
      "Augmented Y28.jpg -> 6 samples\n",
      "Augmented Y104.jpg -> 6 samples\n",
      "Augmented Y85.JPG -> 6 samples\n",
      "Augmented Y26.jpg -> 6 samples\n",
      "Augmented Y161.JPG -> 6 samples\n",
      "Augmented Y120.JPG -> 6 samples\n",
      "Augmented Y97.JPG -> 6 samples\n",
      "Augmented Y44.JPG -> 6 samples\n",
      "Augmented Y79.jpg -> 6 samples\n",
      "Augmented Y36.JPG -> 6 samples\n",
      "Augmented Y17.jpg -> 6 samples\n",
      "Augmented Y117.JPG -> 6 samples\n",
      "Augmented Y16.JPG -> 6 samples\n",
      "Augmented Y165.JPG -> 6 samples\n",
      "Augmented Y187.jpg -> 6 samples\n",
      "Augmented Y27.jpg -> 6 samples\n",
      "Augmented Y90.jpg -> 6 samples\n",
      "Augmented Y39.jpg -> 6 samples\n",
      "Augmented Y112.JPG -> 6 samples\n",
      "Augmented Y252.jpg -> 6 samples\n",
      "Augmented Y3.jpg -> 6 samples\n",
      "Augmented Y42.jpg -> 6 samples\n",
      "Augmented Y101.jpg -> 6 samples\n",
      "Augmented Y54.jpg -> 6 samples\n",
      "Augmented Y66.JPG -> 6 samples\n",
      "Augmented Y99.JPG -> 6 samples\n",
      "Augmented Y31.jpg -> 6 samples\n",
      "Augmented Y29.jpg -> 6 samples\n",
      "Augmented Y193.JPG -> 6 samples\n",
      "Augmented Y6.jpg -> 6 samples\n",
      "Augmented Y19.JPG -> 6 samples\n",
      "Augmented Y167.JPG -> 6 samples\n",
      "Augmented Y166.JPG -> 6 samples\n",
      "Augmented Y73.jpg -> 6 samples\n",
      "Augmented Y51.jpg -> 6 samples\n",
      "Augmented Y242.JPG -> 6 samples\n",
      "Augmented Y243.JPG -> 6 samples\n",
      "Augmented Y74.jpg -> 6 samples\n",
      "Augmented Y113.JPG -> 6 samples\n",
      "Augmented Y181.jpg -> 6 samples\n",
      "Augmented Y70.jpg -> 6 samples\n",
      "Augmented Y251.JPG -> 6 samples\n",
      "Augmented Y185.jpg -> 6 samples\n",
      "Augmented Y98.JPG -> 6 samples\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "tasks = [\n",
    "    {\"file_dir\": image_dir + \"yes\", \"n_generated_samples\": 6, \"save_to_dir\": augmented_data_path + \"yes\"},\n",
    "    {\"file_dir\": image_dir + \"no\", \"n_generated_samples\": 9, \"save_to_dir\": augmented_data_path + \"no\"},\n",
    "]\n",
    "\n",
    "def run_augmentation(task):\n",
    "    augment_data(\n",
    "        file_dir=task[\"file_dir\"],\n",
    "        n_generated_samples=task[\"n_generated_samples\"],\n",
    "        save_to_dir=task[\"save_to_dir\"],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(run_augmentation, task) for task in tasks]\n",
    "    for future in futures:\n",
    "        future.result()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_brain_contour(image, blur_kernel=(5, 5), threshold=45, plot=False):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    gray = cv2.GaussianBlur(gray, blur_kernel, 0)\n",
    "\n",
    "    thresh = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "\n",
    "    if not cnts:\n",
    "        raise ValueError(\"No contours found. Check the input image or thresholding parameters.\")\n",
    "\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "    # Crop the image using the extreme points\n",
    "    new_image = image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]\n",
    "\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(new_image)\n",
    "        plt.title('Cropped Image')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn.utils import shuffle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def load_data(dir_list, image_size, crop_func=crop_brain_contour, progress_bar=False):\n",
    "    \n",
    "    image_width, image_height = image_size\n",
    "    \n",
    "    def process_image(directory, filename):\n",
    "        try:\n",
    "            image_path = f\"{directory}/{filename}\"\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"Warning: Failed to load image {filename}\")\n",
    "                return None, None\n",
    "\n",
    "            if crop_func:\n",
    "                image = crop_func(image)\n",
    "\n",
    "            image = cv2.resize(image, (image_width, image_height), interpolation=cv2.INTER_CUBIC)\n",
    "            image = image / 255.0\n",
    "\n",
    "            label = [1] if 'yes' in directory else [0]\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for directory in dir_list:\n",
    "            for filename in listdir(directory):\n",
    "                futures.append(executor.submit(process_image, directory, filename))\n",
    "        \n",
    "\n",
    "        for future in futures:\n",
    "            image, label = future.result()\n",
    "            if image is not None:\n",
    "                X.append(image)\n",
    "                y.append(label)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Shuffle data\n",
    "    X, y = shuffle(X, y)\n",
    "\n",
    "    # Print dataset information\n",
    "    print(f'Number of examples: {len(X)}')\n",
    "    print(f'X shape: {X.shape}')\n",
    "    print(f'y shape: {y.shape}')\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2065\n",
      "X shape: (2065, 240, 240, 3)\n",
      "y shape: (2065, 1)\n"
     ]
    }
   ],
   "source": [
    "augmented_yes =augmented_data_path+'yes'\n",
    "augmented_no = augmented_data_path+'no'\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = (240, 240)\n",
    "\n",
    "X, y = load_data([augmented_yes, augmented_no], (IMG_WIDTH, IMG_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.2):\n",
    "       \n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_size)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input_shape):\n",
    "    activation_function = 'relu'\n",
    "    pool_size = (2, 2)\n",
    "    kernel_size = (3, 3)\n",
    "    \n",
    "    def conv_block(inputs, filters, activation=activation_function, pool_size=pool_size, kernel_size=kernel_size):\n",
    "        \"\"\"Helper function to create a convolutional block with Conv2D, MaxPooling, and BatchNormalization.\"\"\"\n",
    "        x = Conv2D(filters, kernel_size, activation=activation, padding='same')(inputs)\n",
    "        x = MaxPooling2D(pool_size)(x)\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "        return x\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = conv_block(inputs, 32)\n",
    "    x = conv_block(x, 32)\n",
    "    x = conv_block(x, 64)\n",
    "    x = conv_block(x, 128)\n",
    "    x = conv_block(x, 256)\n",
    "    x = conv_block(x, 512)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(256, activation=activation_function)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(128, activation=activation_function)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_28          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_30          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_31          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_24 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_25 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_26 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_27 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_28 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_28          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_29 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,179,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_30          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_31          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,796,385</span> (10.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,796,385\u001b[0m (10.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,793,569</span> (10.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,793,569\u001b[0m (10.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,816\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "model=cnn_model(IMG_SHAPE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = 'best_model2.keras'\n",
    "\n",
    "\n",
    "checkpoint_loss = ModelCheckpoint(\n",
    "    'best_loss_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_acc = ModelCheckpoint(\n",
    "    'best_accuracy_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr1 = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.7220 - loss: 0.5993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 02:24:29.731759: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion', 76 bytes spill stores, 76 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_reduce_select_fusion_5', 68 bytes spill stores, 68 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_reduce_reduce_window_fusion_5', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.72924, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.44839, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 176ms/step - accuracy: 0.7233 - loss: 0.5968 - val_accuracy: 0.4484 - val_loss: 1.7292 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8500 - loss: 0.3425\n",
      "Epoch 2: val_loss did not improve from 1.72924\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.44839\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.8515 - loss: 0.3403 - val_accuracy: 0.4484 - val_loss: 1.7648 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9357 - loss: 0.1801\n",
      "Epoch 3: val_loss improved from 1.72924 to 1.33364, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.44839 to 0.49032, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9356 - loss: 0.1797 - val_accuracy: 0.4903 - val_loss: 1.3336 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9663 - loss: 0.1016\n",
      "Epoch 4: val_loss improved from 1.33364 to 0.80020, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.49032 to 0.57097, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9659 - loss: 0.1023 - val_accuracy: 0.5710 - val_loss: 0.8002 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9406 - loss: 0.1427\n",
      "Epoch 5: val_loss did not improve from 0.80020\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.57097 to 0.65484, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 0.9410 - loss: 0.1416 - val_accuracy: 0.6548 - val_loss: 0.9310 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9529 - loss: 0.1257\n",
      "Epoch 6: val_loss did not improve from 0.80020\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.65484\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.9534 - loss: 0.1244 - val_accuracy: 0.4871 - val_loss: 2.4657 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9773 - loss: 0.0616\n",
      "Epoch 7: val_loss improved from 0.80020 to 0.26251, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.65484 to 0.90968, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9772 - loss: 0.0620 - val_accuracy: 0.9097 - val_loss: 0.2625 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9776 - loss: 0.0531\n",
      "Epoch 8: val_loss did not improve from 0.26251\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.90968\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.9777 - loss: 0.0528 - val_accuracy: 0.7387 - val_loss: 0.8580 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9876 - loss: 0.0334\n",
      "Epoch 9: val_loss improved from 0.26251 to 0.23318, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.90968 to 0.93226, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9875 - loss: 0.0336 - val_accuracy: 0.9323 - val_loss: 0.2332 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9916 - loss: 0.0223\n",
      "Epoch 10: val_loss improved from 0.23318 to 0.18939, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.93226 to 0.93548, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9917 - loss: 0.0222 - val_accuracy: 0.9355 - val_loss: 0.1894 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9954 - loss: 0.0200\n",
      "Epoch 11: val_loss improved from 0.18939 to 0.14139, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.93548 to 0.93871, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9952 - loss: 0.0204 - val_accuracy: 0.9387 - val_loss: 0.1414 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9863 - loss: 0.0402\n",
      "Epoch 12: val_loss did not improve from 0.14139\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.93871\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.9862 - loss: 0.0405 - val_accuracy: 0.8839 - val_loss: 0.4216 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9541 - loss: 0.1072\n",
      "Epoch 13: val_loss did not improve from 0.14139\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.93871\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.9545 - loss: 0.1066 - val_accuracy: 0.6806 - val_loss: 1.6962 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9410 - loss: 0.1524\n",
      "Epoch 14: val_loss did not improve from 0.14139\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.93871\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.9416 - loss: 0.1508 - val_accuracy: 0.9065 - val_loss: 0.3613 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9683 - loss: 0.0916\n",
      "Epoch 15: val_loss improved from 0.14139 to 0.12315, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.93871 to 0.96452, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.9686 - loss: 0.0909 - val_accuracy: 0.9645 - val_loss: 0.1232 - learning_rate: 8.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9846 - loss: 0.0403\n",
      "Epoch 16: val_loss did not improve from 0.12315\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.96452\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9847 - loss: 0.0400 - val_accuracy: 0.7548 - val_loss: 0.8891 - learning_rate: 8.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9733 - loss: 0.0628\n",
      "Epoch 17: val_loss improved from 0.12315 to 0.11888, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.96452\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.9735 - loss: 0.0628 - val_accuracy: 0.9645 - val_loss: 0.1189 - learning_rate: 8.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9780 - loss: 0.0666\n",
      "Epoch 18: val_loss did not improve from 0.11888\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.96452\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9781 - loss: 0.0663 - val_accuracy: 0.9355 - val_loss: 0.1549 - learning_rate: 8.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9477 - loss: 0.1300\n",
      "Epoch 19: val_loss did not improve from 0.11888\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.96452\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9480 - loss: 0.1293 - val_accuracy: 0.8516 - val_loss: 0.4405 - learning_rate: 8.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9931 - loss: 0.0301\n",
      "Epoch 20: val_loss improved from 0.11888 to 0.10883, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.96452\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.9930 - loss: 0.0302 - val_accuracy: 0.9581 - val_loss: 0.1088 - learning_rate: 8.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9977 - loss: 0.0168\n",
      "Epoch 21: val_loss improved from 0.10883 to 0.09048, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.96452 to 0.96774, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.9977 - loss: 0.0166 - val_accuracy: 0.9677 - val_loss: 0.0905 - learning_rate: 8.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9965 - loss: 0.0151\n",
      "Epoch 22: val_loss improved from 0.09048 to 0.07413, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.96774 to 0.97419, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - accuracy: 0.9965 - loss: 0.0151 - val_accuracy: 0.9742 - val_loss: 0.0741 - learning_rate: 8.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9973 - loss: 0.0172\n",
      "Epoch 23: val_loss did not improve from 0.07413\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.97419\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9973 - loss: 0.0171 - val_accuracy: 0.9452 - val_loss: 0.1615 - learning_rate: 8.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9185 - loss: 0.1992\n",
      "Epoch 24: val_loss did not improve from 0.07413\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.97419\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9179 - loss: 0.2003 - val_accuracy: 0.7452 - val_loss: 1.2753 - learning_rate: 8.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9561 - loss: 0.1355\n",
      "Epoch 25: val_loss did not improve from 0.07413\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.97419\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.9563 - loss: 0.1352 - val_accuracy: 0.8581 - val_loss: 0.3933 - learning_rate: 8.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9683 - loss: 0.1065\n",
      "Epoch 26: val_loss did not improve from 0.07413\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.97419\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9686 - loss: 0.1053 - val_accuracy: 0.9484 - val_loss: 0.1648 - learning_rate: 6.4000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9959 - loss: 0.0233\n",
      "Epoch 27: val_loss did not improve from 0.07413\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.97419\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9959 - loss: 0.0231 - val_accuracy: 0.9677 - val_loss: 0.1133 - learning_rate: 6.4000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9896 - loss: 0.0297\n",
      "Epoch 28: val_loss did not improve from 0.07413\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.97419\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9895 - loss: 0.0298 - val_accuracy: 0.9613 - val_loss: 0.1432 - learning_rate: 6.4000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9888 - loss: 0.0418\n",
      "Epoch 29: val_loss did not improve from 0.07413\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.97419\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9889 - loss: 0.0415 - val_accuracy: 0.9742 - val_loss: 0.0840 - learning_rate: 5.1200e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9970 - loss: 0.0191\n",
      "Epoch 30: val_loss improved from 0.07413 to 0.06881, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.97419\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.9970 - loss: 0.0190 - val_accuracy: 0.9742 - val_loss: 0.0688 - learning_rate: 5.1200e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9904 - loss: 0.0249\n",
      "Epoch 31: val_loss did not improve from 0.06881\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.97419\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9906 - loss: 0.0248 - val_accuracy: 0.9323 - val_loss: 0.2036 - learning_rate: 5.1200e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9986 - loss: 0.0082\n",
      "Epoch 32: val_loss did not improve from 0.06881\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.97419\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9985 - loss: 0.0083 - val_accuracy: 0.9677 - val_loss: 0.1111 - learning_rate: 5.1200e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0060\n",
      "Epoch 33: val_loss did not improve from 0.06881\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.97419\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9742 - val_loss: 0.0950 - learning_rate: 5.1200e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9971 - loss: 0.0069\n",
      "Epoch 34: val_loss improved from 0.06881 to 0.06260, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.97419 to 0.97742, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.9971 - loss: 0.0070 - val_accuracy: 0.9774 - val_loss: 0.0626 - learning_rate: 4.0960e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9978 - loss: 0.0086\n",
      "Epoch 35: val_loss did not improve from 0.06260\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.97742\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9978 - loss: 0.0086 - val_accuracy: 0.9774 - val_loss: 0.0783 - learning_rate: 4.0960e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0046\n",
      "Epoch 36: val_loss did not improve from 0.06260\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.97742\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9742 - val_loss: 0.0712 - learning_rate: 4.0960e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 37: val_loss did not improve from 0.06260\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.97742\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9774 - val_loss: 0.0708 - learning_rate: 4.0960e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9980 - loss: 0.0043\n",
      "Epoch 38: val_loss did not improve from 0.06260\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.97742\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9980 - loss: 0.0043 - val_accuracy: 0.9742 - val_loss: 0.0734 - learning_rate: 3.2768e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 39: val_loss did not improve from 0.06260\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.97742\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9742 - val_loss: 0.0724 - learning_rate: 3.2768e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 40: val_loss did not improve from 0.06260\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.97742\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9742 - val_loss: 0.0706 - learning_rate: 3.2768e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 41: val_loss did not improve from 0.06260\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.97742\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.9999 - loss: 0.0015 - val_accuracy: 0.9742 - val_loss: 0.0746 - learning_rate: 2.6214e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9942 - loss: 0.0251\n",
      "Epoch 42: val_loss did not improve from 0.06260\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.97742\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9941 - loss: 0.0253 - val_accuracy: 0.9742 - val_loss: 0.0933 - learning_rate: 2.6214e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9992 - loss: 0.0073\n",
      "Epoch 43: val_loss improved from 0.06260 to 0.06099, saving model to best_loss_model.keras\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.97742 to 0.98065, saving model to best_accuracy_model.keras\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9992 - loss: 0.0072 - val_accuracy: 0.9806 - val_loss: 0.0610 - learning_rate: 2.6214e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 44: val_loss did not improve from 0.06099\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.98065\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.9774 - val_loss: 0.0688 - learning_rate: 2.6214e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 45: val_loss did not improve from 0.06099\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.98065\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.9774 - val_loss: 0.0689 - learning_rate: 2.6214e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 46: val_loss did not improve from 0.06099\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.98065\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9774 - val_loss: 0.0677 - learning_rate: 2.6214e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9991 - loss: 0.0054\n",
      "Epoch 47: val_loss did not improve from 0.06099\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.98065\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9991 - loss: 0.0055 - val_accuracy: 0.9710 - val_loss: 0.0970 - learning_rate: 2.0972e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 48: val_loss did not improve from 0.06099\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.98065\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9710 - val_loss: 0.0864 - learning_rate: 2.0972e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 49: val_loss did not improve from 0.06099\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.98065\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.9742 - val_loss: 0.0791 - learning_rate: 2.0972e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9956 - loss: 0.0059\n",
      "Epoch 50: val_loss did not improve from 0.06099\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.98065\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9957 - loss: 0.0060 - val_accuracy: 0.9774 - val_loss: 0.0740 - learning_rate: 1.6777e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7c67a01714c0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_val, y_val),\n",
    "                callbacks=[checkpoint_loss, checkpoint_acc, reduce_lr1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9836 - loss: 0.0636\n",
      "Test Accuracy: 97.42%\n",
      "Test loss: 9.14%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Test loss: {loss * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step\n",
      "Accuracy: 0.97\n",
      "Recall: 0.96\n",
      "Recall: 0.99\n",
      "F1-Score: 0.98\n",
      "-------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       147\n",
      "           1       0.99      0.96      0.98       163\n",
      "\n",
      "    accuracy                           0.97       310\n",
      "   macro avg       0.97      0.97      0.97       310\n",
      "weighted avg       0.97      0.97      0.97       310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "\n",
    "predicted_classes = (pred > 0.5).astype(\"int32\")\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "recall = recall_score(y_test, predicted_classes)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "recall = precision_score(y_test, predicted_classes)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, predicted_classes)\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print('-------------------------------')\n",
    "\n",
    "report = classification_report(y_test, predicted_classes)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
